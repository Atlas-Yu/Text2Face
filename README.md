# SARA-Text2Face
Author: Atlas Yu, Yoichi Matsuyama


### What is SARA-Text2Face
SARA-Text2Face is a project to generate facial action units based on sentences, rapport and conversational strategies.

### What does SARA-Text2Face do
 - Clean and process RAPT 2016 Data.		
 - Visualize, analyze, and train on RAPT 2016 data.

### Background
 - [OpenFace](https://github.com/TadasBaltrusaitis/OpenFace) 
 - [Action Units](https://github.com/TadasBaltrusaitis/OpenFace/wiki/Action-Units)

### Project set up

To set up the project, create a Python virtual environment in the project folder. And install all the packages using:
   

`pip install -r requirements.txt`

	

# Data processing

This secssion accomplish the following tasks:
1. Combining Non-verbal behaviors, sentences, words, rapport level, conversational strategies (CS) and tasks for each dyad in each session.
2. Matching Action Units (AU) data extracted from OpenFace to corrensponding dyads and sessions.
3. Mapping AUs to each word based on:
	- Average value of each AU for all the AUs frames within the word. **(--avg)**
	- Median value of each AU for all the AUs frames within the word. **(--median)**
	- Max value of each AU for all the AUs frames within the word. **(--max)** !!This method might be abusing the data.
	- Sum value of all the AUs within each frame, and then pick the frame with highest AUs value for the word. **(--sum)**
	- Matching Conversational Strategies to each word which has AUs values assigned.
	
	
### 1. Combining 
The RAPT 2016 data is located in the **/Data** folder. **/nvb, /words, /rapport** each contains data for Non-verbal behaviors, words, and rapport&CS&sentences. Some files were originally in .txt format. To ease data processing, all the files were converted into .csv format using */Scripts/data_processing/TXTtoCSV.py*

To Combine data in the above folders, navigate to the root folder, run the following command:

`python RAPTCleaner.py --all`

This will generate combined data for each dyad in each session in **/Data/Combined** folder. All the dyad and sessions are also combined into a single file RAPT2016_FULL.csv
	
### 2. Process raw AUs
AUs generated by OpenFace for each dyad session are seperated into 30 seconds slices. Each CSV file in the session corresponds to one slice. These raw files are located in **/Data/au/slices**
To combine all the slices for each dyad session, navigate to the root folder, run the following command:

`python RAPTCleaner.py --slices`

This will generate files containing all the au values for each dyad session in **/Data/au/combined**
	
	
### 3. Match AUs to Word
After the second step, another step is necessary to use AUs. Since AUs are annotated by OpenFace for 30fps, each word corresponds to different numbers of AUs frames. To find the AUs frame which best represent the word, four matching methods are explored, as indicated above.
	
To match AUs to each word based on **average**, run:
		
`python RAPTCleaner.py --avg`
		
To match AUs to each word based on **median**, run:
		
`python RAPTCleaner.py --median`
		
To match AUs to each word based on **max**, run:
		
`python RAPTCleaner.py --max`
		
To match AUs to each word based on **max of sum**, run:
		
`python RAPTCleaner.py --sum`
		
The above commands will generate data in each of the folders below:
		/Data/au/words_avg
		/Data/au/words_median
		/Data/au/words_max
		/Data/au/words_sum
		
	
### 4. Match CS to word
After all of the above steps, the program will assign rapport level and conversational strategies to each word for later training purposes. This step can be done ONLY after all of the above steps.
	
To assign CS to each word for all types of AUs combinging method, run:
	
`python RAPTCleaner.py --cs`
	
Alternatively, if you do not wish to use all types of combinging methods.
To assign CS to each word for a single type of AUs combining method, run one of the commands below:
	
`python RAPTCleaner.py --cs --avg`

`python RAPTCleaner.py --cs --median`

`python RAPTCleaner.py --cs --max`

`pythone RAPTCleaner.py --cs --sum`



# Analyzing

This section accomplish the following tasks:
1. Visualizing data distribution for each session in a browser, capable of interaction.
2. Clustering and visualizing AUs for each dyad session.
3. Generate word embeddings from RAPT, or prepare Glove for training.
4. Linear regression for generating single AU values using word embeddings and CS.
5. LSTM module to generate AUs based on word embeddings and CS.
	
	
### 1. Visualizing RAPT 2016 data distribution
This module plot data from each dyad session in gantt chart, which shows distribution of all the sentences, words, CS, Non-verbal behaviors, etc. in a single graph through out time. PCA (Principal component analysis) is used to reduce data into 2 dimensions.
To plot gantt charts, run the following command:
		
`python RAPTAnalyzer.py [--plot] [--filename file.csv] [--segments int]`
		
Parameters: 

**--filename**: *required*. file for a single dyad session (example: D10_S1.csv). These are the files generated by the RAPTClearner in the above session, which are located in /Data/Combined

**--segments**: *required*. how many segments to divide the selected session to plot. For example, --segments 10 will generate 10 gantt charts for the session. Recommend selecting a number >10. Otherwise each gantt chart will contain too much data to observe any detail.
								
	
### 2. Clustering and visualizing AUs
 There are a variety of clustering methods for testing. However, due to the large volume of data, best clustering results are more likely to show for each social and task session within a dyad session.
 
To plot clusters, run the following command:
		
`python RAPTAnalyzer.py [--cluster] [--filename file.csv] [--begin int] [--end int] [--mode String] [--method String] [--partner String] [--n_cluster int] [--min_size int] [--AU String ...]`
		
Parameters: 

**--filename**: *required*. file for a single dyad session (example: "D10_S1.csv"). These are the files generated by the RAPTClearner in the above session, which are located in /Data/au/words_*

**--begin**: *optional*. the beginning index of the data segment in the dyad session file. Needs to be paired with --end.

**--end**: *optional*. the end index of the data segment in the dyad session file. Needs to be paired with --begin

**--mode**: *required*. The mothod for mapping AUs to words, which indicate data folder. Choices are one of **["avg","median","sum","max"]**

**--method**: *required*. The clustering method. Choices are one of **["K-Means", "AP", "MS", "SC", "AC", "DBSCAN", "HDBSCAN"]**

>NOTE:
> - "K-Means": K-means
> - "AP": Affinity Propagation (might consume too much memory)
>-  "MS": Mean Shift (not efficient)
> - "SC": Spectral Clustering (might consume too much memory)
> - "AC": Agglomerative Clustering (might consume too much memory)
> - "DBSCAN": See reference*
> - "HDBSCAN": See reference* (Best result so far)

**--partner**: *required*. which partner for clustering. ["P1", "P2"]

**--n_cluster**: *option*. Only required if you are using K-means, SC, or AC

**--min_size**: *option*. Only required if you are using HDBSCAN

**--AUs**: *required*. List of AUs for clustering. Choose any number from the list: 
				**['AU01_r','AU02_r','AU04_r','AU05_r','AU06_r','AU07_r','AU09_r','AU10_r','AU12_r','AU14_r','AU15_r','AU17_r','AU20_r','AU23_r','AU25_r','AU26_r','AU45_r']**
					
*[Reference](https://hdbscan.readthedocs.io/en/latest/comparing_clustering_algorithms.html) 
		
	
### 3. Generating word embeddings
The module is capable of training word embeddings from RAPT 2016 data. However, as the resulting vocabulary size is only a few hundreds, it is not recommended to use them for training. Use existing embeddings from Glove instead.

To train embeddings:
		
`python RAPTAnalyzer.py [--embd] [--size int] [--min_count int] [--glove bool]`
		
Parameters:

**--size**: *required*. The size of vector for each word. recommend 50 only.

**--min_count**: *required*. Minimum times a word occurs to be considered into vocabulary.

**--glove**: *optional*. If to use glove embeddings instead.

	
### 4. Linear regression and visualization
The linear regression module train on all sessions for a single AU and plot the generation result.

To run linear regression:
		
`python RAPTAnalyzer.py [--linear] [--mode String] [--embedding embedding.txt.word2vec] [--au String]`
		
Parameters:

**--mode**: *required*. The mothod for mapping AUs to words, which indicate data folder. Choices are one of **["avg","median","sum","max"]**

**--embedding**: *required*. Embedding file for training. These files are stored in /Data/word_embeddings. The available Glove files are **['glove.6B.100d.txt.word2vec', 'glove.6B.50d.txt.word2vec']**
	
**--au**: *required*. Select one au for linear regression. Choose one from 
					**['AU01_r','AU02_r','AU04_r','AU05_r','AU06_r','AU07_r','AU09_r','AU10_r','AU12_r','AU14_r','AU15_r','AU17_r','AU20_r','AU23_r','AU25_r','AU26_r','AU45_r']**
					
	
### 5. Long short-term memory (LSTM) 
In order to train an lstm, the module need to prepare data for the module first to reduce computation time.

To generate data for LSTM:
		
`python RAPTAnalyzer.py [--LSTMData] [--mode String]`
		
Parameters: 

**--mode**: *required*. The mothod for mapping AUs to words, which indicate data folder. Choices are one of **["avg","median","sum","max"]**
		
The generated data will be stored in **/Data/lstm_data**. Now we are ready to train LSTM modules:
		
`python RAPTAnalyzer.py [--LSTM] [--epochs int] [--cs_sel int] [--with_cs bool]`
		
Parameters: 

**--epochs**: *required*. Number of epochs for training.

**--cs_sel**: *required*. Which CS used for training. Options are one of **[-1, 0, 1, 2]** (-1=all CS, 0=rapport, 1=praise, 2=self disclosure)

**--with_cs**: *required*. If to train with CS.
		
After training, the module performs a t-test for indivisual AUs generated from LSTMs trained with CS and without CS using test data.

The current LSTM module does not support batch training.
		
